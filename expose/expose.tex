%%
%% Beuth Hochschule für Technik --  Abschlussarbeit
%%
%% Hauptdokument
%%
%% 23.01.09 Tschirley V.01
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt, a4paper]{book}
\newcommand*{\refname}{Bibliography}
%% Übersetzen als Entwurf
%\usepackage[entwurf]{bhtThesis}
%% Übersetzen für die Abgabe
\usepackage[abgabe]{latexTemplate/bhtThesis}
\typeout{BHT-Abschlussarbeit V.02 15.02.12 S.Tschirley}

\usepackage{blindtext}   %für Blindtext in Kapitel 2
\usepackage{listings}
\lstset{ 
  literate={ö}{{\"o}}1
           {ä}{{\"a}}1
           {ü}{{\"u}}1
           {Ö}{{\"O}}1
           {Ä}{{\"A}}1
           {Ü}{{\"U}}1
           {ß}{{\ss}}1
}

%%
%% Es folgen einige Zusätze, die in Kapitel 1 beschriben sind. 
%% Alles was nicht notwendig ist, kann auskommentiert werden
%%
\usepackage{trsym}
%\usepackage{showkeys}
\usepackage{bytefield}
\usepackage{longtable, booktabs, colortbl}
\usepackage{xstring}% Needed for \IfStrEqCase
\usepackage{datenumber}% Date formatting
\usepackage{advdate}% Needed for \AdvanceDate
\usepackage{pgf}% For math
\usepackage[ddmmyyyy]{datetime}
\usepackage{etoolbox}
\usepackage{pgfkeys,pgfcalendar}

\newdate{datum}{27}{04}{2014}
\newdate{startdatum}{28}{04}{2014}
\newcounter{weekMultiplier}
\newcounter{dayNumber}
\newcounter{offset}
\newcount\pgfdatecount


\newcommand{\nextMonday}[1]{%
\setcounter{weekMultiplier}{#1}
\multiply\value{weekMultiplier} by 7
%\pgfmathsetcounter{offset}{int(mod(\number\getdateday{startdatum}+1,7))}
\pgfmathsetcounter{offset}{int(mod((\number\getdatemonth{startdatum}) + (\number\getdateday{startdatum}) + 1, 7))}
\setcounter{dayNumber}{\getdateday{startdatum}-\value{offset}+\value{weekMultiplier}-2}
\pgfcalendardatetojulian{\getdateyear{startdatum}-\getdatemonth{startdatum}-\value{dayNumber}}{\pgfdatecount}
\pgfcalendarjuliantodate{\the\pgfdatecount}{\myyear}{\mymonth}{\myday}
 \myday.\mymonth.\myyear
}




%%
%% Pfad zu den Bildern
%%
\graphicspath{
  {pictures/},
  {einleitung/pictures},
  {kapitel1/pictures/},
  {kapitel2/pictures/}
}

%%
%% Einbinden persönlicher macros 
%%
%\input{personalMacros.tex}

%% Message
\typeout{-----------------------------------------------------------}
\typeout{----> main.tex ---- Zentrales Dokument---------------------}
\typeout{-----------------------------------------------------------}

\version{0.1$\alpha$}


%%
%% Titel, Autor und Betreuer
%%
\fachbereich{Technik} 
\studiengang{Medieninformatik-Online (Master)}
\autor{Sascha P. Lorenz}
\mtrnr{501 63 21}
\emailAdresse{sascha.lorenz@technik-emden.de}
\titel{Big Data Processing mit Apache Spark} 
\untertitel{}
\thesistyp{Masterarbeit}
\abschluss{Master of Science (M.Sc.)}
\betreuerFeld{

  \begin{tabular}{llr}
    \textbf{1. Betreuer} & Prof. Dr. Stefan Edlich \\
		\textbf{Gutachter} & Prof. Dr. Schiemann-Lillie \\
  \end{tabular}
}




%%\renewcommand{\baselinestretch}{1.05} 
\makeatletter
\renewenvironment{thebibliography}[1]
     {\section{\bibname}% <-- this line was changed from \chapter* to \section*
      \@mkboth{\MakeUppercase\bibname}{\MakeUppercase\bibname}%
      \list{\@biblabel{\@arabic\c@enumiv}}%
           {\settowidth\labelwidth{\@biblabel{#1}}%
            \leftmargin\labelwidth
            \advance\leftmargin\labelsep
            \@openbib@code
            \usecounter{enumiv}%
            \let\p@enumiv\@empty
            \renewcommand\theenumiv{\@arabic\c@enumiv}}%
      \sloppy
      \clubpenalty4000
      \@clubpenalty \clubpenalty
      \widowpenalty4000%
      \sfcode`\.\@m}
     {\def\@noitemerr
       {\@latex@warning{Empty `thebibliography' environment}}%
      \endlist}
\makeatother


\begin{document}
\maketitle
~
\newpage

\pagestyle{empty}
\pagenumbering{arabic}
\clearpage\pagenumbering{roman}
\clearpage\pagenumbering{arabic}
\pagestyle{fancy}
\setcounter{page}{1}
\setcounter{chapter}{1}
%\setcounter{section}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Die Kapitel

%\chapter{}
\vspace{10ex}


\section{Einleitung}
Gegenstand dieser Arbeit sind die Grundlagen der Verarbeitung und Analyse großer Datenmengen (Big Data). Zunächst sollen verschiedene Ansätze mit Ihren Funktionsweisen sowie den Vor- und Nachteilen diskutiert werdem. Hier werden zuerst allgemeine Grundlagen zu Big Data erarbeitet. Was ist Big Data, was unterscheidet die Verarbeitung von strukturierten und unstrukturierten Daten, Relationale Datenbanken vs. noSQL, wie müssen die Quelldaten für die jeweiligen Verarbeitungen beschaffen sein, welche besonderen Herausforderungen stellen gestreamte Daten an die Verarbeitung. Besonders wird hier auf Hadoop und den Map/Reduce-Algorithmus eingegangen, um das bisher etablierte Vorgehen zu beschreiben und ein grundsätzliches Verständnis für die Domäne "Big Data Processing" zu schaffen. In diesem Kontext wird das gesamte Ökosystem rund um Hadoop vorgestellt. 

Für diesen Teil der Arbeit werden entsprechende Versuche besonders zu den Themen 
Quelldatenbeschaffenheit (sowie Beschaffung und Verfügbarmachung), Streaming vs. Persistent und dem Hadoop-Ökosystem auf dem Cluster und Lokal durchgeführt und dokumentiert.
(Hinweis für Herrn Schiemann-Lillie: an der Beuth-HS Berlin wurde ein starkes Rechner-Cluster konfiguriert, um dort unter anderem diese Versuche mit Spark und dessen Ökosystem durchführen zu können)

Nachdem eine Einführung in das Thema "Big Data Processing" erfolgt ist und ein entsprechend quantitativ und qualitativ brauchbarer Datensatz zur Verfügung steht, werden die Next-Generation Data-Processing Technologien betrachtet. Kernthema ist hier Apache Spark und der gesamte BDAS (Berkeley Data Analytics Stack), der von den den AMP-Labs innerhalb von Apache-Projekten um Spark herum aufgebaut wurde. Jedes der Bestandteile wird innerhalb dieser Arbeit betrachtet und in Versuchen auf dem Cluster erprobt. Zu praktisch jeder "offiziellen" BDAS-Implementierung existieren noch Alternativen. Diese werden ermittelt, gegebenenfalls erprobt und die Unterschiede und Gemeinsamkeiten dokumentiert. Besonders Laufzeiten sind in diesem Umfeld von größter Wichtigkeit. Hier werden entsprechend besonders intensive Versuche durchgeführt, zum Beispiel, in dem auf großen Datensätzen Map/Reduce- oder Logostic-Regression-Algorithmen zum Einsatz kommen. Besonders Apache flink wird hier als Alternative näher untersucht. Auch Applikationen, die auf dem eigentlichen Stack aufsetzen, werden näher betrachtet und entsprechenden Praxistests unterzogen (beispielsweise H2O für statistische Analysen). 

Danach wird die API von Spark und deren Möglichkeiten mit Scala, Java und Clojure näher betrachtet und durch jeweils eigene Implementierungen untersucht. 

Die Arbeit schließt mit durch verschiedene Versuchsreihen fundierte Empfehlungen für die unterschiedlichen Anforderungen im Bereich des Big-Data-Processing.




\end{document}
